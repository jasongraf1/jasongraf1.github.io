{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Tutorial Part 1: Introduction\n",
    "\n",
    "spaCy is a free, open-source library for Natural Language Processing (NLP) in Python. There are many things it can do to help you process language data for all kinds of uses. \n",
    "\n",
    "### What is spaCy good for?\n",
    "\n",
    "spaCy is great for quickly analyzing natural language data, especially (standard) written language data, using pretrained models. Common tasks such as tokenization, POS-tagging, dependency parsing, Named Entity Recognition, and word embeddings/vector-space semantic modelling can be done incredibly quickly using the pre-trained models.  \n",
    "\n",
    "### What is spaCy **not** for?\n",
    "\n",
    "Strickly speaking, spaCy is not designed for actual NLP research, unlike other libraries such as [Natural Language Toolkit (NLTK)](https://github.com/nltk/nltk). It is designed more for speedy application and production rather than the development and evaluation of optimal NLP algorithms. The aim with spaCy is to simply get things done.\n",
    "\n",
    "Fortunately, spaCy is **very** good at getting things done, which makes it very useful for corpus linguists. You can train your own models, or update the pretrained ones, with your own data, but I will not go into that in these tutorials. At the moment, getting a good model (with the current methods) usually requires *lots* of data, and training on even small datasets is more than a typical personal laptop or computer can handle. You can find more information about training spaCy models [here](https://spacy.io/usage/training)    \n",
    "\n",
    "## Installing spaCy\n",
    "\n",
    "More coming soon. You can find information about installing spaCy at [https://spacy.io/usage](https://spacy.io/usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding language models\n",
    "\n",
    "spaCy comes with pretrained statistical models for a number of languages. There are models of different sizes for about a dozen or so languages, mostly the major European ones, though models for Chinese (Mandarin?) and Japanese are also available. All these models are deep neural networks trained on different kinds of data. You can find a list of spaCy's language models [here](https://spacy.io/usage/models#languages). \n",
    "\n",
    "For English there are three pretrained models of different sizes: small `sm`, medium `md`, large `lg`. All the pretrained English models were trained on [OntoNotes](https://catalog.ldc.upenn.edu/LDC2013T19)), and the larger two models contain context-specific token vectors (20k and 685k vectors, respectively). \n",
    "\n",
    "For most of the tutorials we'll be using the small English model, `en_core_web_sm`, however we will occasionally use the medium-sized model, so we'll download that too. In most cases the small model is good enough, but we'll examine a case where using a larger model makes a big difference.\n",
    "\n",
    "***Note:*** *The pretrained models may not perform well on data that is very different from the kind(s) of data the model was trained on. For instance, models trained on edited written language (newspaper articles, novels, academic texts, etc.) will generally not do well on other kinds of language, e.g. casual spoken conversation or text messages. It's always important to be aware of the limitations of these models.*\n",
    "\n",
    "spaCy’s models can be downloaded in the Python console like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ python -m spacy download en_core_web_sm\n",
    "$ python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the model with the `spacy.load()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_sm = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with `Doc` objects\n",
    "\n",
    "spaCy works on objects of `Doc` class, which contain all kinds of information about a bit of text. To create a `Doc` from a text, we simply run our model over a string of text like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a hole in the ground there lived a hobbit.\n"
     ]
    }
   ],
   "source": [
    "hobbit_doc1 = nlp_sm(\"In a hole in the ground there lived a hobbit.\")\n",
    "\n",
    "print(hobbit_doc1) # you can also simply type 'hobbit_doc1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start doing things with `hobbit_doc1`. \n",
    "\n",
    "### Getting tokens\n",
    "\n",
    "One of the most basic things we do when analyzing texts is **tokenization**. Tokenization refers to the splitting of a text into individual units for further analysis. In most cases, we use it informally to refer to splitting a text into words. \n",
    "The nice thing is that spaCy does this automatically for us when it creates a model object. A `Doc` object is essentially a sequence of `token` objects.\n",
    "\n",
    "We can see the tokens in `hobbit_doc1` by simply iterating over its elements. The individual tokens are themselves spaCy objects of class `token`, and therefore have lots of attributes which contain information about them. Here we use `.text` to get the plain language text of each token and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "a\n",
      "hole\n",
      "in\n",
      "the\n",
      "ground\n",
      "there\n",
      "lived\n",
      "a\n",
      "hobbit\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in hobbit_doc1:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'a',\n",
       " 'hole',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'there',\n",
       " 'lived',\n",
       " 'a',\n",
       " 'hobbit',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list comprehension style\n",
    "[t.text for t in hobbit_doc1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full list of token attributes [here](https://spacy.io/api/token#attributes). Below are some attributes that I think are particularly useful.\n",
    "\n",
    "- `.i`: The index (position) of the token in the text. Starts at 0.\n",
    "- `.text`: The original word text.\n",
    "- `.lemma_`: The base form of the word.\n",
    "- `.pos_`: The simple [Universal Part-Of-Speech tag](https://universaldependencies.org/docs/u/pos/).\n",
    "- `.tag_`: The detailed part-of-speech tag. (I *believe* these are the same tags used in the [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), but I am not sure.) \n",
    "- `.dep_`: Syntactic dependency, i.e. the relation between tokens.\n",
    "- `.shape_`: The word shape – capitalization, punctuation, digits.\n",
    "- `.is_alpha`: Is the token an alpha character?\n",
    "- `.is_punct`: Is the token a punctuation marker?\n",
    "- `.is_stop`: Is the token part of a stop list, i.e. the most common words of the language?\n",
    "\n",
    "You can of course use spaCy with other Python modules to do what you want to do. For example, I'll use the `pandas` module to create a `DataFrame` object, which is automatically printed neatly in jupyter. \n",
    "\n",
    "***Python Note:*** *If you plan to use Python for data analysis, I highly recommend checking out the [`pandas` library](https://pandas.pydata.org/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>pos_</th>\n",
       "      <th>tag_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>INDEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hole</td>\n",
       "      <td>hole</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ground</td>\n",
       "      <td>ground</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>there</td>\n",
       "      <td>there</td>\n",
       "      <td>PRON</td>\n",
       "      <td>EX</td>\n",
       "      <td>expl</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lived</td>\n",
       "      <td>live</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hobbit</td>\n",
       "      <td>hobbit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text  lemma_   pos_ tag_   dep_  is_alpha  INDEX\n",
       "0       In      in    ADP   IN   prep      True      0\n",
       "1        a       a    DET   DT    det      True      1\n",
       "2     hole    hole   NOUN   NN   pobj      True      2\n",
       "3       in      in    ADP   IN   prep      True      3\n",
       "4      the     the    DET   DT    det      True      4\n",
       "5   ground  ground   NOUN   NN   pobj      True      5\n",
       "6    there   there   PRON   EX   expl      True      6\n",
       "7    lived    live   VERB  VBD   ROOT      True      7\n",
       "8        a       a    DET   DT    det      True      8\n",
       "9   hobbit  hobbit   NOUN   NN   dobj      True      9\n",
       "10       .       .  PUNCT    .  punct     False     10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hobbit_tab = [[token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_alpha, token.i] for token in hobbit_doc1]\n",
    "\n",
    "pd.DataFrame(hobbit_tab, columns = [\"text\", \"lemma_\", \"pos_\", \"tag_\", \"dep_\", \"is_alpha\", \"INDEX\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of these attributes have an underscore `_` following them. These tell spaCy to return a label rather than an integer value. If you leave off the `_`, you get a number corresponding to the requisite label (e.g. 90 = 'DET'). These are used internally by spaCy but may not be that useful for you if you don't know what each number refers to. Hence the underscored versions above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 ADP In\n",
      "90 DET a\n",
      "92 NOUN hole\n",
      "85 ADP in\n",
      "90 DET the\n",
      "92 NOUN ground\n",
      "95 PRON there\n",
      "100 VERB lived\n",
      "90 DET a\n",
      "92 NOUN hobbit\n",
      "97 PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in hobbit_doc1:\n",
    "    print(token.pos, token.pos_, token.text) # see the integer and label values for 'pos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**more about custom tokenizers?**\n",
    "\n",
    "\n",
    "### Splitting a text into sentences\n",
    "\n",
    "You can also easily segment a text into sentences, which can be further analyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This hobbit was a very well-to-do hobbit, and his name was Baggins. The Bagginses had lived in the neighbourhood of The Hill for time out of mind, and people considered them very respectable, not only because most of them were rich, but also because they never had any adventures or did anything unexpected: you could tell what a Baggins would say on any question without the bother of asking him. This is a story of how a Baggins had an adventure, and found himself doing and saying things altogether unexpected. He may have lost the neighbours' respect, but he gained—well, you will see whether he gained anything in the end."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hobbit_doc2 = nlp_sm(\"\"\"This hobbit was a very well-to-do hobbit, and his name was Baggins. The Bagginses had lived in the neighbourhood of The Hill for time out of mind, and people considered them very respectable, not only because most of them were rich, but also because they never had any adventures or did anything unexpected: you could tell what a Baggins would say on any question without the bother of asking him. This is a story of how a Baggins had an adventure, and found himself doing and saying things altogether unexpected. He may have lost the neighbours' respect, but he gained—well, you will see whether he gained anything in the end.\"\"\")\n",
    "\n",
    "hobbit_doc2 # The text is printed as an unbroken string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `Doc` objects have an attribute `.sents`, even if there is only one sentence. As with tokenization, sentence segmentation is automatically done when a `Doc` object is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a hole in the ground there lived a hobbit.\n"
     ]
    }
   ],
   "source": [
    "for s in hobbit_doc1.sents:\n",
    "    print(s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This hobbit was a very well-to-do hobbit, and his name was Baggins.\n",
      "The Bagginses had lived in the neighbourhood of The Hill for time out of mind, and people considered them very respectable, not only because most of them were rich, but also because they never had any adventures or did anything unexpected: you could tell what a Baggins would say on any question without the bother of asking him.\n",
      "This is a story of how a Baggins had an adventure, and found himself doing and saying things altogether unexpected.\n",
      "He may have lost the neighbours' respect, but he gained—\n",
      "well, you will see whether he gained anything in the end.\n"
     ]
    }
   ],
   "source": [
    "for s in hobbit_doc2.sents:\n",
    "    print(s.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More about sentences and spans...**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
