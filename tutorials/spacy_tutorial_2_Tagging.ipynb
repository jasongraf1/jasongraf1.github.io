{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Tutorial Part 2: Part-of-Speech tagging and custom tokenization\n",
    "\n",
    "In this session we'll look at tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_sm = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Tokenizing refers to the splitting of a text into individual units for further analysis. In most cases, we use it informally to refer to splitting a text into words. The nice thing is that spaCy does this automatically for us when it creates a model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gregor = nlp_sm(\"One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\")\n",
    "gregor.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the tokens by simply iterating over the elements in `gregor`. Remember that these elements are spaCy objects themselves, and therefore have lots of attributese.g. `.text`, that we can call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "morning\n",
      ",\n",
      "when\n",
      "Gregor\n",
      "Samsa\n",
      "woke\n",
      "from\n",
      "troubled\n",
      "dreams\n",
      ",\n",
      "he\n",
      "found\n",
      "himself\n",
      "transformed\n",
      "in\n",
      "his\n",
      "bed\n",
      "into\n",
      "a\n",
      "horrible\n",
      "vermin\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in gregor:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the full list of token attributes [here](https://spacy.io/api/token#attributes). Below are some attributes that I think are particularly useful.\n",
    "\n",
    "- `i`: The index (position) of the token in the text. Starts at 0.\n",
    "- `text`: The original word text.\n",
    "- `lemma_`: The base form of the word.\n",
    "- `pos_`: The simple [UPOS](https://universaldependencies.org/docs/u/pos/) part-of-speech tag.\n",
    "- `tag_`: The detailed part-of-speech tag. (I *believe* these are the same tags used in the [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), but I am not sure.) \n",
    "- `dep_`: Syntactic dependency, i.e. the relation between tokens.\n",
    "- `shape_`: The word shape – capitalization, punctuation, digits.\n",
    "- `is_alpha`: Is the token an alpha character?\n",
    "- `is_punct`: Is the token a punctuation marker?\n",
    "- `is_stop`: Is the token part of a stop list, i.e. the most common words of the language?\n",
    "\n",
    "You can of course use spaCy with other Python modules to do what you want to do. For example, I'll use the `pandas` module to create a `DataFrame` object, which is automatically printed neatly in jupyter. \n",
    "\n",
    "***Python Note***: If you plan to use Python for data analysis, I highly recommend checking out [`pandas`](https://pandas.pydata.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>pos_</th>\n",
       "      <th>tag_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>INDEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One</td>\n",
       "      <td>one</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>nummod</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>morning</td>\n",
       "      <td>morning</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>npadvmod</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>ADV</td>\n",
       "      <td>WRB</td>\n",
       "      <td>advmod</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gregor</td>\n",
       "      <td>Gregor</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Samsa</td>\n",
       "      <td>Samsa</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>woke</td>\n",
       "      <td>wake</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>advcl</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>troubled</td>\n",
       "      <td>troubled</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dreams</td>\n",
       "      <td>dream</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>pobj</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>he</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>found</td>\n",
       "      <td>find</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>himself</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>transformed</td>\n",
       "      <td>transform</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>his</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>DET</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>poss</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bed</td>\n",
       "      <td>bed</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>True</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>into</td>\n",
       "      <td>into</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>horrible</td>\n",
       "      <td>horrible</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vermin</td>\n",
       "      <td>vermin</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>True</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>False</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text     lemma_   pos_  tag_      dep_  is_alpha  INDEX\n",
       "0           One        one    NUM    CD    nummod      True      0\n",
       "1       morning    morning   NOUN    NN  npadvmod      True      1\n",
       "2             ,          ,  PUNCT     ,     punct     False      2\n",
       "3          when       when    ADV   WRB    advmod      True      3\n",
       "4        Gregor     Gregor  PROPN   NNP     nsubj      True      4\n",
       "5         Samsa      Samsa  PROPN   NNP     nsubj      True      5\n",
       "6          woke       wake   VERB   VBD     advcl      True      6\n",
       "7          from       from    ADP    IN      prep      True      7\n",
       "8      troubled   troubled    ADJ    JJ      amod      True      8\n",
       "9        dreams      dream   NOUN   NNS      pobj      True      9\n",
       "10            ,          ,  PUNCT     ,     punct     False     10\n",
       "11           he     -PRON-   PRON   PRP     nsubj      True     11\n",
       "12        found       find   VERB   VBD      ROOT      True     12\n",
       "13      himself     -PRON-   PRON   PRP     nsubj      True     13\n",
       "14  transformed  transform   VERB   VBN     ccomp      True     14\n",
       "15           in         in    ADP    IN      prep      True     15\n",
       "16          his     -PRON-    DET  PRP$      poss      True     16\n",
       "17          bed        bed   NOUN    NN      pobj      True     17\n",
       "18         into       into    ADP    IN      prep      True     18\n",
       "19            a          a    DET    DT       det      True     19\n",
       "20     horrible   horrible    ADJ    JJ      amod      True     20\n",
       "21       vermin     vermin   NOUN    NN      pobj      True     21\n",
       "22            .          .  PUNCT     .     punct     False     22"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gregor_tab = [[token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_alpha, token.i] for token in gregor]\n",
    "\n",
    "pd.DataFrame(gregor_tab, columns = [\"text\", \"lemma_\", \"pos_\", \"tag_\", \"dep_\", \"is_alpha\", \"INDEX\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of these have an underscore `_` following them. These tell spaCy to return a label rather than an integer value. If you leave off the `_`, you get a slightly different output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One NUM 93\n",
      "morning NOUN 92\n",
      ", PUNCT 97\n",
      "when ADV 86\n",
      "Gregor PROPN 96\n",
      "Samsa PROPN 96\n",
      "woke VERB 100\n",
      "from ADP 85\n",
      "troubled ADJ 84\n",
      "dreams NOUN 92\n",
      ", PUNCT 97\n",
      "he PRON 95\n",
      "found VERB 100\n",
      "himself PRON 95\n",
      "transformed VERB 100\n",
      "in ADP 85\n",
      "his DET 90\n",
      "bed NOUN 92\n",
      "into ADP 85\n",
      "a DET 90\n",
      "horrible ADJ 84\n",
      "vermin NOUN 92\n",
      ". PUNCT 97\n"
     ]
    }
   ],
   "source": [
    "for token in gregor:\n",
    "    print(token.text, token.pos_, token.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with POS tags and other attributes\n",
    "\n",
    "Now that we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way— in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dickens_doc1 = nlp_sm(\"\"\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way— in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\"\"\")\n",
    "dickens_doc1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step might be to get some basic frequency statistics. \n",
    "\n",
    "- The total number of tokens\n",
    "- The number of word tokens, not including punctuation\n",
    "- The number of unique word types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total N tokens\n",
    "len(dickens_doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total N words\n",
    "n_words = 0\n",
    "for word in dickens_doc1:\n",
    "    if not word.is_punct:\n",
    "        n_words = n_words + 1\n",
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:*** Find another way to calculate the total number of words using the `len()` function and list comprehension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{It,\n",
       " was,\n",
       " the,\n",
       " best,\n",
       " of,\n",
       " times,\n",
       " it,\n",
       " was,\n",
       " the,\n",
       " worst,\n",
       " of,\n",
       " times,\n",
       " it,\n",
       " was,\n",
       " the,\n",
       " age,\n",
       " of,\n",
       " wisdom,\n",
       " it,\n",
       " was,\n",
       " the,\n",
       " age,\n",
       " of,\n",
       " foolishness,\n",
       " it,\n",
       " was,\n",
       " the,\n",
       " epoch,\n",
       " of,\n",
       " belief,\n",
       " it,\n",
       " was,\n",
       " the,\n",
       " epoch,\n",
       " of,\n",
       " incredulity,\n",
       " it,\n",
       " was,\n",
       " the,\n",
       " season,\n",
       " of,\n",
       " Light,\n",
       " it,\n",
       " was,\n",
       " the,\n",
       " season,\n",
       " of,\n",
       " Darkness,\n",
       " it,\n",
       " was,\n",
       " the,\n",
       " spring,\n",
       " of,\n",
       " hope,\n",
       " it,\n",
       " was,\n",
       " the,\n",
       " winter,\n",
       " of,\n",
       " despair,\n",
       " we,\n",
       " had,\n",
       " everything,\n",
       " before,\n",
       " us,\n",
       " we,\n",
       " had,\n",
       " nothing,\n",
       " before,\n",
       " us,\n",
       " we,\n",
       " were,\n",
       " all,\n",
       " going,\n",
       " direct,\n",
       " to,\n",
       " Heaven,\n",
       " we,\n",
       " were,\n",
       " all,\n",
       " going,\n",
       " direct,\n",
       " the,\n",
       " other,\n",
       " way,\n",
       " in,\n",
       " short,\n",
       " the,\n",
       " period,\n",
       " was,\n",
       " so,\n",
       " far,\n",
       " like,\n",
       " the,\n",
       " present,\n",
       " period,\n",
       " that,\n",
       " some,\n",
       " of,\n",
       " its,\n",
       " noisiest,\n",
       " authorities,\n",
       " insisted,\n",
       " on,\n",
       " its,\n",
       " being,\n",
       " received,\n",
       " for,\n",
       " good,\n",
       " or,\n",
       " for,\n",
       " evil,\n",
       " in,\n",
       " the,\n",
       " superlative,\n",
       " degree,\n",
       " of,\n",
       " comparison,\n",
       " only}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total N word types\n",
    "word_list = []\n",
    "for word in dickens_doc1:\n",
    "    if not word.is_punct:\n",
    "        word_list.append(word)\n",
    "set(word_list) # set() creates a set '{}' of the unique elements in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(word_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:*** Calculate the total number of *lemma* types. \n",
    "\n",
    "***Exercise:*** Calculate the total number of word types using list comprehension and the `set()` and `len()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a list of words by frequency. This is where using other modules comes in very handy. So we can use the `Counter()` function in the `collections` module to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({It: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         best: 1,\n",
       "         of: 1,\n",
       "         times: 1,\n",
       "         it: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         worst: 1,\n",
       "         of: 1,\n",
       "         times: 1,\n",
       "         it: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         age: 1,\n",
       "         of: 1,\n",
       "         wisdom: 1,\n",
       "         it: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         age: 1,\n",
       "         of: 1,\n",
       "         foolishness: 1,\n",
       "         it: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         epoch: 1,\n",
       "         of: 1,\n",
       "         belief: 1,\n",
       "         it: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         epoch: 1,\n",
       "         of: 1,\n",
       "         incredulity: 1,\n",
       "         it: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         season: 1,\n",
       "         of: 1,\n",
       "         Light: 1,\n",
       "         it: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         season: 1,\n",
       "         of: 1,\n",
       "         Darkness: 1,\n",
       "         it: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         spring: 1,\n",
       "         of: 1,\n",
       "         hope: 1,\n",
       "         it: 1,\n",
       "         was: 1,\n",
       "         the: 1,\n",
       "         winter: 1,\n",
       "         of: 1,\n",
       "         despair: 1,\n",
       "         we: 1,\n",
       "         had: 1,\n",
       "         everything: 1,\n",
       "         before: 1,\n",
       "         us: 1,\n",
       "         we: 1,\n",
       "         had: 1,\n",
       "         nothing: 1,\n",
       "         before: 1,\n",
       "         us: 1,\n",
       "         we: 1,\n",
       "         were: 1,\n",
       "         all: 1,\n",
       "         going: 1,\n",
       "         direct: 1,\n",
       "         to: 1,\n",
       "         Heaven: 1,\n",
       "         we: 1,\n",
       "         were: 1,\n",
       "         all: 1,\n",
       "         going: 1,\n",
       "         direct: 1,\n",
       "         the: 1,\n",
       "         other: 1,\n",
       "         way: 1,\n",
       "         in: 1,\n",
       "         short: 1,\n",
       "         the: 1,\n",
       "         period: 1,\n",
       "         was: 1,\n",
       "         so: 1,\n",
       "         far: 1,\n",
       "         like: 1,\n",
       "         the: 1,\n",
       "         present: 1,\n",
       "         period: 1,\n",
       "         that: 1,\n",
       "         some: 1,\n",
       "         of: 1,\n",
       "         its: 1,\n",
       "         noisiest: 1,\n",
       "         authorities: 1,\n",
       "         insisted: 1,\n",
       "         on: 1,\n",
       "         its: 1,\n",
       "         being: 1,\n",
       "         received: 1,\n",
       "         for: 1,\n",
       "         good: 1,\n",
       "         or: 1,\n",
       "         for: 1,\n",
       "         evil: 1,\n",
       "         in: 1,\n",
       "         the: 1,\n",
       "         superlative: 1,\n",
       "         degree: 1,\n",
       "         of: 1,\n",
       "         comparison: 1,\n",
       "         only: 1})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with POS tags and see how many of each POS we have in the text. We'll use the same `Counter()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'PRON': 18,\n",
       "         'AUX': 16,\n",
       "         'DET': 17,\n",
       "         'ADJ': 9,\n",
       "         'ADP': 20,\n",
       "         'NOUN': 25,\n",
       "         'PROPN': 2,\n",
       "         'ADV': 5,\n",
       "         'VERB': 4,\n",
       "         'SCONJ': 2,\n",
       "         'CCONJ': 1})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = [w.pos_ for w in dickens_doc1 if not w.is_punct]\n",
    "pos_counts = Counter(pos_list)\n",
    "pos_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, `pos_counts` is an object of `Counter` class which is a subclass of `dict`, so it has all the methods of `dict` class including `.keys()` (the POS labels) and `.values()` (their counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON',\n",
       " 'AUX',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PROPN',\n",
       " 'ADV',\n",
       " 'VERB',\n",
       " 'SCONJ',\n",
       " 'CCONJ']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pos_counts.keys()) # list() converts it to a list object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 16, 17, 9, 20, 25, 2, 5, 4, 2, 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pos_counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine them into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRON</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUX</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DET</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADP</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ADV</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VERB</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SCONJ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      POS  Freq\n",
       "0    PRON    18\n",
       "1     AUX    16\n",
       "2     DET    17\n",
       "3     ADJ     9\n",
       "4     ADP    20\n",
       "5    NOUN    25\n",
       "6   PROPN     2\n",
       "7     ADV     5\n",
       "8    VERB     4\n",
       "9   SCONJ     2\n",
       "10  CCONJ     1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = pd.DataFrame(list(zip(pos_counts.keys(), pos_counts.values())), columns = [\"POS\", \"Freq\"])\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADP</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRON</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DET</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUX</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ADV</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VERB</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SCONJ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      POS  Freq\n",
       "5    NOUN    25\n",
       "4     ADP    20\n",
       "0    PRON    18\n",
       "2     DET    17\n",
       "1     AUX    16\n",
       "3     ADJ     9\n",
       "7     ADV     5\n",
       "8    VERB     4\n",
       "6   PROPN     2\n",
       "9   SCONJ     2\n",
       "10  CCONJ     1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.sort_values(\"Freq\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting a text into sentences\n",
    "\n",
    "Below is the text from the first two paragraphs of *Harry Potter and the Philosopher's Stone* (or *Harry Potter and the Sorcerer's Stone* for Americans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense. Mr Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large moustache. Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_text1 = \"\"\"Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense. Mr Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large moustache. Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\"\"\"\n",
    "hp_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a doc object out of larger bits of text as well as individual sentences. We can all the text with the `.text` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense. Mr Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large moustache. Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_doc1 = nlp_sm(hp_text1)\n",
    "hp_doc1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\n",
      "They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\n",
      "Mr Dursley was the director of a firm called Grunnings, which made drills.\n",
      "He was a big, beefy man with hardly any neck, although he did have a very large moustache.\n",
      "Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours.\n",
      "The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\n"
     ]
    }
   ],
   "source": [
    "for s in hp_doc1.sents:\n",
    "    print(s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was on the corner of the street that he noticed the first sign of something peculiar – a cat reading a map. For a second, Mr Dursley didn’t realise what he had seen – then he jerked his head around to look again.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_text2 = \"\"\"It was on the corner of the street that he noticed the first sign of something peculiar – a cat reading a map. For a second, Mr Dursley didn’t realise what he had seen – then he jerked his head around to look again.\"\"\"\n",
    "hp_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was on the corner of the street that he noticed the first sign of something peculiar – a cat reading a map.\n",
      "For a second, Mr Dursley didn’t realise what he had seen – then he jerked his head around to look again.\n"
     ]
    }
   ],
   "source": [
    "hp_doc2 = nlp_sm(hp_text2)\n",
    "\n",
    "for s in hp_doc2.sents:\n",
    "    print(s.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Python Note:*** As with individual words, it's often useful to organise your sentences into a `list` object for use down the line. [List comprehensions](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python) are a very efficient way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing your sentence segmenter\n",
    "\n",
    "It's worth noting that the default spaCy sentence splitting process relies on dependency parses to determine sentence boundaries. This is different from a simpler, though less accurate, rule-based method, e.g. splitting sentences wherever you find a `.`, `?`, or `!`. So this means that you need a statistical model to parse the text for the segmenting to work, which is why we load the model in before we do anything else.\n",
    "\n",
    "However, a model is only as good as the data it was trained on, and if your data is of a very different kind compared to the training data, your segmentation may not work so well. You may find that if you are working with certain kinds of data, e.g. conversational data or data from IM, chats or social media, the default model doesn't work so well. Fortunately, you can customize the segmentation process by adding your own rules to tell spaCy what to do in special cases. What rules you add will depend on the nature of your data.\n",
    "\n",
    "So consider the toy example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding rule: ['this is a sentence...and another...and another sentence.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # create a new model object so I don't alter the main one\n",
    "\n",
    "text = \"this is a sentence...and another...and another sentence.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Before adding rule:\", [sent.text for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to tell spaCy to add a sentence boundary whenever it sees a \"...\" in the text. We do this by defining a function that goes through a `Doc` object token by token and wherever it finds a \"...\", it adds a flag to the following token indicating that it is the beginning of a new sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding rule: ['this is a sentence...', 'and another...', 'and another sentence.']\n"
     ]
    }
   ],
   "source": [
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]: # search from the first to the second to last token\n",
    "        if token.text == \"...\":\n",
    "            # if current token == \"...\", add sent_start flag to the next token (token.i + 1)\n",
    "            doc[token.i + 1].is_sent_start = True \n",
    "    return doc\n",
    "\n",
    "# Now add the new function to the pipeline\n",
    "nlp.add_pipe(set_custom_boundaries, before=\"parser\")\n",
    "\n",
    "doc = nlp(text)\n",
    "print(\"After adding rule:\", [sent.text for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
